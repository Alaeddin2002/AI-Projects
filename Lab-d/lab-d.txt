import numpy as np

def sigmoid(x):
    return 1/(1 + np.e ** (-x))

def init_weights_biases(num_input_nodes, num_hidden_nodes, num_output_nodes):
    parameter_dictionary = {}
#     parameter_dictionary['hidden_biases'] = np.array([[1.4],[-198.2]])
#     parameter_dictionary['output_biases'] = np.array([[6.8]])
#     parameter_dictionary['hidden_weights'] = np.array([[220,-220],[197.8,-398.2]])
#     parameter_dictionary['output_weights'] = np.array([[-16.7,39.1]])
    parameter_dictionary['hidden_biases'] = np.zeros((num_hidden_nodes,1))
    parameter_dictionary['output_biases'] = np.zeros((num_output_nodes,1))
    parameter_dictionary['hidden_weights'] = np.random.randn(num_hidden_nodes,num_input_nodes)
    parameter_dictionary['output_weights'] = np.random.randn(num_output_nodes,num_hidden_nodes)
    return parameter_dictionary 
  
#print(init_weights_biases(2,2,1))

def read_file_to_array(file):
    feature = []
    label = []
    headers = []
    read_file = open(file, 'r')
    lines = read_file.readlines()
    for j in range(0,int(len(lines[1])/2)-1):
        feature.append([])
    for i in range(1,len(lines)):
        temp_list = []
        for j in range(0,int(len(lines[1])/2)-1):
            if lines[i][j] == '\t':
                temp_list += lines[i][j+1]
            else:
                temp_list += lines[i][j]
        for i in range(len(temp_list)):
            x = float(temp_list[i])
            feature[i].append (x)
    for i in range(len(feature)):
        feature_array = np.array([feature])

    ################################
    for i in range(len(lines)):
        if lines[i][-2] == 't':
            continue
        else:
            label.append(float(lines[i][-2]))
    for i in range(len(label)):
        label_array = np.array([[label]])

    ###################################
    for i in range(0,int(len(lines[1])/2)):
        headers.append([])
    headers[0].append(lines[0][0])
    headers[1].append(lines[0][2])
    headers[2].append('output')
    for i in range(len(headers)):
        header_array = np.array(headers)


    features = feature_array[0]
    labels = label_array[0]
    headers = header_array
    return(features,labels,headers)

#features , labels , headers = read_file_to_array("xor.txt")
#print(features)
#print(labels)
#print(headers)

def forward_propagate(features,parameters):
    output = {}
    array = parameters['hidden_weights']
    hidden_bias = parameters['hidden_biases']
    hidden_layer_values = np.dot(array,features) + hidden_bias
    activation = sigmoid(hidden_layer_values)
    output['hidden_layer_outputs'] = activation

    array_2 = parameters['output_weights']
    hidden_o = parameters['output_biases']
    hidden_layer_outputs = np.dot(array_2,activation)+ hidden_o
    activation_2 = sigmoid(hidden_layer_outputs)
    output['output_layer_outputs'] = activation_2
    return output

#output = forward_propagate([len(features),len(features[0])+len(features[1])],init_weights_biases(2, 2, 1))
#print(output)

def find_loss(output_layer_outputs, labels):
    # The number of examples is the number of columns in labels
    num_examples = labels.shape[1]
    loss = (-1 / num_examples) * np.sum(np.multiply(labels, np.log(output_layer_outputs)) + 
                                        np.multiply(1-labels, np.log(1-output_layer_outputs)))
    return loss

#output = forward_propagate([len(features),len(features[0])+len(features[1])],init_weights_biases(2, 2, 1))
#print(output)

def backprop(feature_array, labels, output_vals, weights_biases_dict, verbose=False):
    if verbose:
        print()
    # We get the number of examples by looking at how many total
    # labels there are. (Each example has a label.)
    num_examples = labels.shape[1]

    # These are the outputs that were calculated by each
    # of our two layers of nodes that calculate outputs.
    hidden_layer_outputs = output_vals["hidden_layer_outputs"]
    output_layer_outputs = output_vals["output_layer_outputs"]
    # These are the weights of the arrows coming into our output
    # node from each of the hidden nodes. We need these to know
    # how much blame to place on each hidden node.
    output_weights = weights_biases_dict["output_weights"]
    # This is how wrong we were on each of our examples, and in
    # what direction. If we have four training examples, there
    # will be four of these.
    # This calculation works because we are using binary cross-entropy,
    # which produces a fairly simply calculation here.
    raw_error = output_layer_outputs - labels
    if verbose:
        print("raw_error", raw_error)

    # This is where we calculate our gradient for each of the
    # weights on arrows coming into our output.
    output_weights_gradient = np.dot(raw_error, hidden_layer_outputs.T)/num_examples
    if verbose:
        print("output_weights_gradient", output_weights_gradient)

    # This is our gradient on the bias. It is simply the
    # mean of our errors.
    output_bias_gradient = np.sum(raw_error, axis=1, keepdims=True)/num_examples
    if verbose:
        print("output_bias_gradient", output_bias_gradient)

    # We now calculate the amount of error to propagate back to our hidden nodes.
    # First, we find the dot product of our output weights and the error
    # on each of four training examples. This allows us to figure out how much,
    # for each of our training examples, each hidden node contributed to our
    # getting things wrong.
    blame_array = np.dot(output_weights.T, raw_error)
    if verbose:
        print("blame_array", blame_array)

    # hidden_layer_outputs is the actual values output by our hidden layer for
    # each of the four training examples. We square each of these values.
    hidden_outputs_squared = np.power(hidden_layer_outputs, 2)
    if verbose:
        print("hidden_layer_outputs", hidden_layer_outputs)
        print("hidden_outputs_squared", hidden_outputs_squared)

    # We now multiply our blame array by 1 minus the squares of the hidden layer's
    # outputs.
    propagated_error = np.multiply(blame_array, 1-hidden_outputs_squared)
    if verbose:
        print("propagated_error", propagated_error)

    # Finally, we compute the magnitude and direction in which we
    # should adjust our weights and biases for the hidden node.
    hidden_weights_gradient = np.dot(propagated_error, feature_array.T)/num_examples
    hidden_bias_gradient = np.sum(propagated_error, axis=1,
    keepdims=True)/num_examples
    if verbose:
        print("hidden_weights_gradient", hidden_weights_gradient)
        print("hidden_bias_gradient", hidden_bias_gradient)

    # A dictionary that stores all of the gradients
    # These are values that track which direction and by
    # how much each of our weights and biases should move
    gradients = {"hidden_weights_gradient": hidden_weights_gradient,
    "hidden_bias_gradient": hidden_bias_gradient,
    "output_weights_gradient": output_weights_gradient,
    "output_bias_gradient": output_bias_gradient}
    return gradients

def update_weights_biases(parameter_dictionary, gradients, learning_rate):
    hidden_weights = parameter_dictionary['hidden_weights']
    output_weights = parameter_dictionary['output_weights']
    hidden_bias = parameter_dictionary['hidden_biases']
    output_bias = parameter_dictionary['output_biases']
    hidden_weights_gradient = gradients['hidden_weights_gradient']
    hidden_bias_gradient = gradients['hidden_bias_gradient']
    output_weights_gradient = gradients['output_weights_gradient']
    output_bias_gradient = gradients['output_bias_gradient']
    updated_parameters = {'hidden_weights' : hidden_weights - learning_rate * hidden_weights_gradient,
                          'hidden_biases' : hidden_bias - learning_rate * hidden_bias_gradient,
                          'output_weights' : output_weights - learning_rate * output_weights_gradient,
                          'output_biases' : output_bias - learning_rate * output_bias_gradient}
    return updated_parameters

def model_file(file_name, num_inputs, num_hiddens, num_outputs, epochs, learning_rate):
    feature_array, label_array, header_array = read_file_to_array(file_name)
    parameter_dictionary = init_weights_biases(num_inputs,num_hiddens,num_outputs)
    epoch = 0

    while epoch <= epochs:
        forward_propagate_ = forward_propagate(feature_array,parameter_dictionary)
        output_layer = forward_propagate_['output_layer_outputs']
        loss = find_loss(output_layer,label_array)
        if epoch % 10 == 0:
            print("Loss", loss)
        gradients = backprop(feature_array,label_array,forward_propagate_,parameter_dictionary)
        parameter_dictionary = update_weights_biases(parameter_dictionary, gradients, learning_rate)
        epoch += 1
    return parameter_dictionary

print(model_file('xor.txt',2,2,1,1000,0.001))